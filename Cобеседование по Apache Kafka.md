# Cобеседование Apache Kafka. Разбор вопросов и ответов.


<a href="https://mc.yandex.ru/pixel/8711235002931986822?rnd=%aw_random%">
    <img src="https://mc.yandex.ru/pixel/8711235002931986822?rnd=%aw_random%" />        
  </a>&nbsp;&nbsp;
<a href="https://mc.yandex.ru/watch/92801430">
    <img src="https://mc.yandex.ru/watch/92801430" />        
  </a>&nbsp;&nbsp;


Нажмите ★, если вам нравится проект. Ваш вклад сердечно ♡ приветствуется.

Если вам интересно мое резюме: https://github.com/DEBAGanov


#  Apache Kafka
- [Cобеседование по Apache Kafka. Разбор вопросов и ответов.](#cобеседование-по-java-разбор-вопросов-и-ответов)
- [Базы данных](#базы-данных)
  - [Что такое _«база данных»_?](#что-такое-база-данных)
  - [Что такое _«система управления базами данных»_?](#что-такое-система-управления-базами-данных)
    

[1. Что такое Apache Kafka?] (#1. Что такое Apache Kafka?)

[2. Каковы основные компоненты Kafka?] (#2. Каковы основные компоненты Kafka?)

[3. Что такое Kafka Broker?] (#3. Что такое Kafka Broker?)

[4. Что такое Topic в Kafka?] (#4. Что такое Topic в Kafka?)

[5. Как работает модель публикации/подписки в Kafka?] (#5. Как работает модель публикации/подписки в Kafka?)

[6. Что такое Partition в Kafka и зачем он нужен?] (#6. Что такое Partition в Kafka и зачем он нужен?)

[7. Как Kafka обеспечивает высокую доступность и отказоустойчивость?] (#7. Как Kafka обеспечивает высокую доступность и отказоустойчивость?)

[8. Что такое Consumer Group в Kafka?] (#8. Что такое Consumer Group в Kafka?)

[9. Как происходит балансировка нагрузки между потребителями в группе?] (#9. Как происходит балансировка нагрузки между потребителями в группе?)

[10. Что такое Offset в Kafka?] (#10. Что такое Offset в Kafka?)

[11. Как можно гарантировать порядок сообщений в Kafka?] (#11. Как можно гарантировать порядок сообщений в Kafka?)

[12. Какова роль Zookeeper в Kafka?] (#12. Какова роль Zookeeper в Kafka?)

[13. Что такое Producer в Kafka?] (#13. Что такое Producer в Kafka?)

[14. Как реализовать асинхронную отправку сообщений в Kafka?] (#14. Как реализовать асинхронную отправку сообщений в Kafka?)

[15. Как настроить сериализацию и десериализацию сообщений?] (#15. Как настроить сериализацию и десериализацию сообщений?)

[16. В чем разница между KafkaProducer и KafkaConsumer?] (#16. В чем разница между KafkaProducer и KafkaConsumer?)

[17. Что такое Kafka Streams?] (#17. Что такое Kafka Streams?)

[18. Как использовать Kafka Connect?] (#18. Как использовать Kafka Connect?)

[19. Что такое Retention Policy в Kafka?] (#19. Что такое Retention Policy в Kafka?)

[20. Как можно управлять конфигурацией Kafka?] (#20. Как можно управлять конфигурацией Kafka?)

[21. Что такое Dead Letter Queue (DLQ)
в Kafka?] (#21. Что такое Dead Letter Queue (DLQ)
в Kafka?)

[22. Как реализовать транзакции в Kafka?] (#22. Как реализовать транзакции в Kafka?)

[23. Как производители и потребители обрабатывают ошибки в Kafka?] (#23. Как производители и потребители обрабатывают ошибки в Kafka?)

[24. Каковы основные преимущества использования Kafka?] (#24. Каковы основные преимущества использования Kafka?)

[25. Что такое Kafka Schema Registry?] (#25. Что такое Kafka Schema Registry?)

[26. Как использовать Avro с Kafka?] (#26. Как использовать Avro с Kafka?)

[27. Как обеспечить безопасность в Kafka?] (#27. Как обеспечить безопасность в Kafka?)

[28. Что такое логическая архитектура Kafka?] (#28. Что такое логическая архитектура Kafka?)

[29. Как сделать мониторинг Kafka?] (#29. Как сделать мониторинг Kafka?)

[30. Что такое KSQL?] (#30. Что такое KSQL?)

[31. Как обрабатывать события в реальном времени с помощью Kafka?] (#31. Как обрабатывать события в реальном времени с помощью Kafka?)

[32. Что такое Compaction в Kafka?] (#32. Что такое Compaction в Kafka?)

[33. Как настроить репликацию в Kafka?] (#33. Как настроить репликацию в Kafka?)

[34. Чем отличается acks=all от acks=1?] (#34. Чем отличается acks=all от acks=1?)

[35. Как управлять производительностью Kafka?] (#35. Как управлять производительностью Kafka?)

[36. Что такое Kafka Consumer Lag?] (#36. Что такое Kafka Consumer Lag?)

[37. Как можно отладить Kafka-приложение?] (#37. Как можно отладить Kafka-приложение?)

[38. Что такое Kafka Streams API?] (#38. Что такое Kafka Streams API?)

[39. Как использовать Kafka с Spring Boot?] (#39. Как использовать Kafka с Spring Boot?)

[40. Как реализовать интеграцию Kafka с базой данных?] (#40. Как реализовать интеграцию Kafka с базой данных?)

[41. Что такое Kafka MirrorMaker?] (#41. Что такое Kafka MirrorMaker?)

[42. Как обеспечить обработку событий в порядке их получения?] (#42. Как обеспечить обработку событий в порядке их получения?)

[43. Что такое Kafka REST Proxy?] (#43. Что такое Kafka REST Proxy?)

[44. Как использовать KafkaTemplate в Spring Kafka?] (#44. Как использовать KafkaTemplate в Spring Kafka?)

[45. Как обрабатывать JSON-сообщения в Kafka?] (#45. Как обрабатывать JSON-сообщения в Kafka?)

[46. Что такое Partition Reassignment?] (#46. Что такое Partition Reassignment?)

[47. Как использовать Kafka для микросервисной архитектуры?] (#47. Как использовать Kafka для микросервисной архитектуры?)

[48. Что такое Producer Callback и как его использовать?] (#48. Что такое Producer Callback и как его использовать?)

[49. Как реализовать шифрование сообщений в Kafka?] (#49. Как реализовать шифрование сообщений в Kafka?)

[50. Какие инструменты мониторинга совместимы с Kafka?] (#50. Какие инструменты мониторинга совместимы с Kafka?)




- [Источники](#источники)

Что такое очередь сообщений.
Основные концепции очередей
? Kafka vs Rabbit MQ
Основные сущности Kafka

Zookeper. Хранение метаданных кластера
Kafka кластер. Устройство
Партиционирование. Leader партиция.
Репликация
Настройка Kafka кластера для корректной работы партиционирования и репликации
Устройство файлового хранилища Kafka
TTL
Producer
Producer. Из каких шагов состоит инцициализация
Стратегии коммитинга. Гарантия доставки
Сериализация, Десериализация
Стратегии выбора партиции продюссером
Можно ли из топика (распределен по 3 партициям) прочитать сообщения в том же порядке, в котором они были записаны? Почему?
Как сделать так, чтобы все сообщения по одному клиенту попали в одну партицию?
Timestamp
Headers
Batch size. Linger time
Retry

1. Расскажите мне о ситуации, когда Кафка — не лучший вариант.
2. Как бы вы изменили время удержания в Kafka? 
3. Объясните максимальный размер сообщения, которое может получить Kafka.

4. Сравните Apache Kafka с другой популярной потоковой платформой.

5. Когда бы вы использовали функцию кластера в Kafka?
Как разбалансировать кластер в Kafka?

6. Что бы вы сделали, если бы при использовании Kafka возникла ошибка?

## 7. Как бы вы получили одно сообщение от Кафки во время производства данных?

#### Получение одного сообщения из Kafka

Чтобы получить одно сообщение из Kafka во время производства данных, вам нужно использовать **Kafka Consumer**. Вот основные шаги, которые помогут вам это сделать:

1. **Создание Consumer**: Сначала необходимо создать экземпляр Kafka Consumer, указав необходимые параметры конфигурации, такие как `bootstrap.servers`, `group.id`, и `key.deserializer`, `value.deserializer`.

2. **Подписка на топик**: После создания Consumer, вы должны подписаться на нужный топик, из которого хотите получать сообщения. Это делается с помощью метода `subscribe()`.

3. **Получение сообщения**: Для получения сообщения используйте метод `poll()`. Этот метод будет блокировать выполнение, пока не получит сообщение. Чтобы получить только одно сообщение, вы можете использовать `poll(Duration.ofMillis(100))` и затем обработать полученное сообщение.

4. **Коммит смещения**: После обработки сообщения, если вы хотите зафиксировать смещение, используйте метод `commitSync()`, чтобы сохранить текущее положение в потоке сообщений.

Вот пример кода на Java:

```java
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class KafkaSingleMessageConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("my-topic"));

        // Получение одного сообщения
        ConsumerRecord<String, String> record = consumer.poll(Duration.ofMillis(100)).iterator().next();
        System.out.println("Получено сообщение: " + record.value());

        // Коммит смещения
        consumer.commitSync();
        consumer.close();
    }
}
```

Этот код создаёт Consumer, подписывается на топик и получает одно сообщение. Не забудьте обработать возможные исключения, такие как `NoSuchElementException`, если сообщений нет.

8. Что вы имеете в виду, когда говорите «отказоустойчивость»?

## 9. Как бы вы интегрировали Kafka с другими фреймворками?



1. **Использование Kafka Connect**: Kafka Connect — это инструмент, который позволяет легко интегрировать Kafka с другими системами, такими как базы данных, хранилища данных и другие системы обработки данных. Он поддерживает множество коннекторов, которые могут быть настроены для автоматической передачи данных между Kafka и другими источниками или приемниками данных.

2. **Интеграция с Apache Spark**: Apache Spark может использовать Kafka для обработки потоковых данных. Spark Streaming позволяет обрабатывать данные в реальном времени, получая их из Kafka. Это позволяет создавать мощные приложения для анализа данных, которые могут обрабатывать большие объёмы информации.

3. **Использование с Apache Storm**: Apache Storm также может быть интегрирован с Kafka для обработки потоков данных. Storm позволяет обрабатывать данные в реальном времени и может использовать Kafka как источник данных, что делает его идеальным для приложений, требующих низкой задержки.

4. **Интеграция с REST API**: Kafka может быть использован в сочетании с REST API для передачи данных между различными приложениями. Это позволяет разработчикам создавать приложения, которые могут взаимодействовать с Kafka через стандартные HTTP-запросы.

5. **Подключение к системам мониторинга и аналитики**: Kafka может быть интегрирован с системами мониторинга и аналитики, такими как Elasticsearch и Grafana, для визуализации и анализа потоковых данных в реальном времени.

Эти методы интеграции позволяют использовать возможности Kafka для создания масштабируемых и эффективных систем обработки данных.


1. Что такое Apache Kafka?
   Apache Kafka - это распределенная платформа потоковой передачи данных, которая позволяет публиковать и подписываться на потоки записей. Она разработана для обработки данных в реальном времени и обеспечивает высокую пропускную способность, масштабируемость и надежность.

2. Каковы основные компоненты Kafka?
   Основные компоненты:
- Брокеры (Brokers)
- Производители (Producers)
- Потребители (Consumers)
- Топики (Topics)
- ZooKeeper
- Партиции (Partitions)

3. Что такое Kafka Broker?
   Брокер - это сервер Kafka, который хранит данные и обслуживает запросы клиентов. Кластер Kafka состоит из нескольких брокеров, где каждый имеет уникальный ID.

4. Что такое Topic в Kafka?
   Topic - это категория или канал, в который публикуются записи. Топики могут иметь множество производителей и потребителей. Каждый топик разделен на партиции.

5. Как работает модель публикации/подписки в Kafka?
   Производители публикуют сообщения в топики, а потребители подписываются на эти топики для получения сообщений. Это обеспечивает слабую связанность между отправителями и получателями.

6. Что такое Partition в Kafka и зачем он нужен?
   Партиция - это упорядоченная последовательность сообщений в топике. Партиции позволяют:
- Распределять данные между брокерами
- Обеспечивать параллельную обработку
- Масштабировать производительность

7. Как Kafka обеспечивает высокую доступность и отказоустойчивость?
   Через:
- Репликацию данных
- Распределение партиций между брокерами
- Автоматическое восстановление после сбоев
- Выборы лидера партиции

8. Что такое Consumer Group в Kafka?
   Consumer Group - это группа потребителей, которые совместно обрабатывают сообщения из топиков. Каждое сообщение доставляется только одному потребителю в группе.

9. Как происходит балансировка нагрузки между потребителями в группе?
   Kafka автоматически распределяет партиции между потребителями в группе. При добавлении или удалении потребителя происходит ребалансировка.

10. Что такое Offset в Kafka?
    Offset - это уникальный последовательный идентификатор сообщения в партиции. Потребители используют offset для отслеживания прочитанных сообщений.

11. Как можно гарантировать порядок сообщений в Kafka?
    Порядок сообщений гарантируется только в пределах одной партиции. Для обеспечения порядка нужно:
- Использовать один и тот же ключ партиции для связанных сообщений
- Настроить параметр max.in.flight.requests.per.connection=1
- Использовать подтверждения (acks=all)

12. Какова роль Zookeeper в Kafka?
    ZooKeeper отвечает за:
- Хранение метаданных о кластере
- Выборы контроллера
- Отслеживание состояния брокеров
- Управление квотами и ACL
  Примечание: с версии 3.0 Kafka может работать без ZooKeeper (KRaft).

13. Что такое Producer в Kafka?
    Producer - это клиент, который публикует сообщения в топики Kafka. Основные характеристики:
- Может отправлять сообщения синхронно или асинхронно
- Поддерживает балансировку нагрузки
- Имеет встроенные механизмы сериализации

14. Как реализовать асинхронную отправку сообщений в Kafka?
    Асинхронная отправка реализуется через:
- Использование метода send() с callback
- Настройку параметра batch.size
- Использование producer.flush() при необходимости

15. Как настроить сериализацию и десериализацию сообщений?
    Через:
- Реализацию интерфейсов Serializer и Deserializer
- Настройку key.serializer и value.serializer
- Использование встроенных сериализаторов (String, Integer, etc.)
- Применение форматов как Avro, Protobuf или JSON

16. В чем разница между KafkaProducer и KafkaConsumer?
    KafkaProducer:
- Отправляет сообщения
- Управляет партиционированием
- Поддерживает асинхронную отправку

KafkaConsumer:
- Читает сообщения
- Управляет смещениями
- Поддерживает групповое потребление

17. Что такое Kafka Streams?
    Kafka Streams - это библиотека для потоковой обработки данных, которая позволяет:
- Создавать приложения для обработки потоков
- Выполнять агрегации и соединения
- Обрабатывать события в реальном времени
- Поддерживать состояние приложения

18. Как использовать Kafka Connect?
    Kafka Connect - это фреймворк для интеграции данных, который:
- Поддерживает готовые коннекторы
- Позволяет создавать собственные коннекторы
- Обеспечивает масштабируемость
- Поддерживает распределенный и автономный режимы

19. Что такое Retention Policy в Kafka?
    Retention Policy определяет:
- Как долго хранятся сообщения
- Максимальный размер данных
- Правила очистки старых данных
- Политику компактификации

20. Как можно управлять конфигурацией Kafka?
    Конфигурацией можно управлять через:
- Файлы конфигурации (server.properties)
- Динамические настройки через API
- Переменные окружения
- Инструменты администрирования

21. Что такое Dead Letter Queue (DLQ) в Kafka?
    DLQ - это специальный топик для сообщений, которые не удалось обработать. Используется для:
- Сохранения проблемных сообщений
- Анализа ошибок обработки
- Повторной обработки сообщений
- Мониторинга качества данных

22. Как реализовать транзакции в Kafka?
    Транзакции в Kafka реализуются через:
- Использование TransactionalId
- Инициализацию транзакционного продюсера
- Методы beginTransaction() и commitTransaction()
- Настройку isolation.level для потребителей

23. Как производители и потребители обрабатывают ошибки в Kafka?
    Обработка ошибок включает:
- Retry-механизмы
- Exception handlers
- Dead Letter Queue
- Мониторинг и логирование
- Настройку таймаутов

24. Каковы основные преимущества использования Kafka?
    Основные преимущества:
- Высокая производительность
- Масштабируемость
- Отказоустойчивость
- Долговременное хранение
- Гарантированная доставка сообщений

25. Что такое Kafka Schema Registry?
    Schema Registry - это сервис для управления схемами данных, который:
- Хранит и версионирует схемы
- Обеспечивает совместимость
- Поддерживает Avro, Protobuf, JSON Schema
- Валидирует сообщения

26. Как использовать Avro с Kafka?
    Для использования Avro нужно:
- Определить схему в формате Avro
- Настроить Schema Registry
- Использовать AvroSerializer/AvroDeserializer
- Управлять эволюцией схем

27. Как обеспечить безопасность в Kafka?
    Безопасность обеспечивается через:
- SSL/TLS шифрование
- SASL аутентификацию
- ACL авторизацию
- Аудит доступа
- Шифрование данных

28. Что такое логическая архитектура Kafka?
    Логическая архитектура включает:
- Топики и партиции
- Реплики и лидеры
- Производители и потребители
- Группы потребителей
- Контроллер брокера

29. Как сделать мониторинг Kafka?
    Мониторинг осуществляется через:
- JMX метрики
- Prometheus/Grafana
- Kafka Manager
- Custom метрики
- Логи брокеров

30. Что такое KSQL?
    KSQL - это движок потоковых SQL-запросов для Kafka:
- Позволяет писать SQL-подобные запросы
- Поддерживает агрегации и джойны
- Работает в реальном времени
- Интегрируется с существующими потоками

31. Как обрабатывать события в реальном времени с помощью Kafka?
    Обработка в реальном времени осуществляется через:
- Kafka Streams API
- KSQL
- Низкие задержки доставки
- Параллельную обработку партиций
- Оптимизацию производительности

32. Что такое Compaction в Kafka?
    Compaction - это механизм очистки топиков, который:
- Сохраняет последнее значение для каждого ключа
- Уменьшает размер данных
- Поддерживает изменяемые состояния
- Оптимизирует хранение

33. Как настроить репликацию в Kafka?
    Настройка репликации включает:
- Установку фактора репликации
- Выбор лидера партиции
- Настройку ISR (In-Sync Replicas)
- Управление синхронизацией

34. Чем отличается acks=all от acks=1?
    acks=all:
- Ждет подтверждения от всех реплик
- Максимальная надежность
- Большая латентность

acks=1:
- Ждет подтверждения только от лидера
- Средняя надежность
- Меньшая латентность

35. Как управлять производительностью Kafka?
    Управление производительностью через:
- Настройку параметров брокера
- Оптимизацию партиций
- Конфигурацию продюсеров/потребителей
- Мониторинг метрик
- Балансировку нагрузки

36. Что такое Kafka Consumer Lag?
    Consumer Lag - это отставание потребителя:
- Разница между последним опубликованным и прочитанным сообщением
- Индикатор производительности
- Метрика мониторинга
- Показатель здоровья системы

37. Как можно отладить Kafka-приложение?
    Отладка включает:
- Анализ логов
- Мониторинг метрик
- Использование инструментов отладки
- Тестирование конфигураций
- Проверку консьюмер-групп

38. Что такое Kafka Streams API?
    Kafka Streams API предоставляет:
- DSL для обработки потоков
- Операции над данными
- Управление состоянием
- Масштабируемость
- Отказоустойчивость

39. Как использовать Kafka с Spring Boot?
    Интеграция включает:
- Spring Kafka
- Конфигурацию в application.properties
- KafkaTemplate
- @KafkaListener аннотации
- Обработку ошибок

40. Как реализовать интеграцию Kafka с базой данных?
    Интеграция через:
- Kafka Connect
- CDC (Change Data Capture)
- Пользовательские коннекторы
- Транзакционную обработку

41. Что такое Kafka MirrorMaker?
    Kafka MirrorMaker - это инструмент для репликации данных между кластерами:
- Поддерживает географическую репликацию
- Обеспечивает аварийное восстановление
- Позволяет агрегировать данные
- Поддерживает фильтрацию топиков

42. Как обеспечить обработку событий в порядке их получения?
    Для обеспечения порядка нужно:
- Использовать одну партицию для связанных событий
- Настроить правильный ключ партиционирования
- Использовать временные метки
- Контролировать параллелизм обработки

43. Что такое Kafka REST Proxy?
    Kafka REST Proxy:
- Предоставляет HTTP API для Kafka
- Позволяет работать с Kafka без клиентских библиотек
- Поддерживает форматы JSON/Binary/Avro
- Обеспечивает доступ через веб-протоколы

44. Как использовать KafkaTemplate в Spring Kafka?
    KafkaTemplate используется для:
- Отправки сообщений в топики
- Обработки подтверждений
- Управления транзакциями
- Обработки ошибок отправки

45. Как обрабатывать JSON-сообщения в Kafka?
    Обработка JSON включает:
- Использование JsonSerializer/JsonDeserializer
- Маппинг на Java-объекты
- Валидацию схемы
- Обработку ошибок десериализации

46. Что такое Partition Reassignment?
    Partition Reassignment позволяет:
- Перераспределять партиции между брокерами
- Балансировать нагрузку
- Обрабатывать отказы брокеров
- Оптимизировать использование ресурсов

47. Как использовать Kafka для микросервисной архитектуры?
    Использование в микросервисах:
- Асинхронная коммуникация
- Паттерн Event Sourcing
- CQRS
- Распределенные транзакции
- Обработка отказов

48. Что такое Producer Callback и как его использовать?
    Producer Callback:
- Асинхронная обработка результатов отправки
- Обработка ошибок
- Метрики успешности
- Подтверждение доставки

49. Как реализовать шифрование сообщений в Kafka?
    Шифрование реализуется через:
- SSL/TLS на транспортном уровне
- Шифрование на уровне сообщений
- Пользовательские сериализаторы
- Управление ключами шифрования

50. Какие инструменты мониторинга совместимы с Kafka?
    Инструменты мониторинга:
- Prometheus/Grafana
- Kafka Manager (CMAK)
- JMX-мониторинг
- ELK Stack
- Datadog









[к оглавлению](#Базы-данных)



# Источники
+ [Википедия](https://ru.wikipedia.org/wiki/)
+ [tokarchuk.ru](http://tokarchuk.ru/2012/08/indexes-classification/)
+ [Quizful](http://www.quizful.net/interview/sql/)

[Вопросы для собеседования](README.md)
